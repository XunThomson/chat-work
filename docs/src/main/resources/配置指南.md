# 配置指南

### 一、校验tar是否安装

```bash
yum install -y -tar
```

### 二、解压工具包

```bash
tar -zxvf xxxx.tar.gz 或 tar -xvf xxx.tar
```

### 三、数据库mysql安装

- 安装

```bash
sudo dnf localinstall \
  mysql-community-common-8.4.6-1.el8.x86_64.rpm \
  mysql-community-icu-data-files-8.4.6-1.el8.x86_64.rpm \
  mysql-community-client-plugins-8.4.6-1.el8.x86_64.rpm \
  mysql-community-libs-8.4.6-1.el8.x86_64.rpm \
  mysql-community-client-8.4.6-1.el8.x86_64.rpm \
  mysql-community-server-8.4.6-1.el8.x86_64.rpm
```

- 校验

```bash
mysql --version
```

- 启动

```bash
sudo systemctl start mysqld
#sudo systemctl enable mysqld
```

- 查看初始化默认密码

```
cat /var/log/mysqld.log
```

- 登录&初始化*(条件允许自行查询，定制)*
- MySQL 8.0+ 的安全机制：**首次使用临时密码登录后，会强制要求你修改密码**
- **登录过一次，临时密码已被强制失效**

```
sudo mysql_secure_installation

#大致设置
#--设置 root 密码
#--移除匿名用户
#--禁止 root 远程登录
#--删除测试数据库
#--重新加载权限表

mysql>quit
mysql -u root -p
```

- 禁止远程root后需要操作(选择题)

```bash
#创建一个可以从任意主机连接的用户
CREATE USER 'devuser'@'%' IDENTIFIED BY 'StrongPass123!';

GRANT ALL PRIVILEGES ON *.* TO 'devuser'@'%' WITH GRANT OPTION;

FLUSH PRIVILEGES;

#限制只允许特定 IP 连接
CREATE USER 'devuser'@'192.168.1.100' IDENTIFIED BY 'StrongPass123!';

GRANT ALL PRIVILEGES ON *.* TO 'devuser'@'192.168.1.100' WITH GRANT OPTION;

FLUSH PRIVILEGES;

#只授权某个数据库
CREATE USER 'appuser'@'%' IDENTIFIED BY 'AppPass2025!';

GRANT ALL PRIVILEGES ON myappdb.* TO 'appuser'@'%';

FLUSH PRIVILEGES;
```

- 修改配置文件允许远程连接

```
#编辑 MySQL 配置文件
sudo vi /etc/my.cnf

#注释掉或删除这一行
bind-address = 127.0.0.1

或者显式绑定到所有接口
bind-address = 0.0.0.0
```

- 保存后

```
sudo systemctl restart mysqld
```

- 开放防火墙

```
sudo firewall-cmd --permanent --add-port=3306/tcp
sudo firewall-cmd --reload
```

- 最后测试连接


### 四、配置环境

- 配置java环境变量

```
#解压后进行
#解压到   /opt/java/

#可以重命名为jdk-17

#1. 创建配置文件
sudo vim /etc/profile.d/java.sh

#2. 写入以下内容（根据你的实际路径修改）
export JAVA_HOME=/opt/jdk-17
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar

#3. 保存后,添加执行权限
sudo chmod +x /etc/profile.d/java.sh

#4. 立即生效
source /etc/profile.d/java.sh

#5. 校验

java -version
javac -version
echo $JAVA_HOME

```

- 配置gcc

>  AlmaLinux（或 CentOS、RHEL）系统中配置 `gcc`，**安装 GCC 编译器套件**。因为 `gcc` 通常已经集成在系统路径中，安装后即可直接使用。

```
sudo dnf group install "Development Tools"

sudo yum group install "Development Tools"

gcc --version
```

> - `gcc`（C 编译器）
> - `gcc-c++`（C++ 编译器）
> - `make`
> - `gdb`（调试器）
> - `binutils`（链接器、汇编器等）
> - `kernel-headers`、`glibc-devel` 等开发库

```
#看情况（选择）

# 安装标准 C++ 库头文件
sudo dnf install libstdc++-devel

# 安装数学库（-lm）
sudo dnf install glibc-devel

# 安装 OpenMP 支持
sudo dnf install gcc gcc-c++ libgomp
```

> AlmaLinux（或 CentOS、RHEL）系统中配置 Python，通常不需要手动“配置环境变量”，因为系统自带 Python 或通过包管理器安装后会自动加入 `PATH`。

```
python --version   # 可能是 Python 2.7（旧版）
python3 --version  # 推荐使用
```

- 安装wget

```
sudo dnf install wget

sudo yum install wget

wget --version

--------------------------------------
以下针对使用不了yum的情况(centos7)
sudo yum localinstall wget-1.14-18.el7_6.1.x86_64.rpm

sudo rpm -ivh wget-1.14-18.el7_6.1.x86_64.rpm
```

- 安装uv

```
#使用 curl 下载脚本并通过 sh 执行
curl -LsSf https://astral.sh/uv/install.sh | sh

#如果系统没有 curl，可以使用 wget
wget -qO- https://astral.sh/uv/install.sh | sh

```

> 如果安装没有提示词，那么等待超时结束或者强制结束。然后再来几次，今天不行明天再来。

```
#校验安装
uv
```

### 五、安装vllm(测试部署使用ollama，因为方便)

```
uv venv --python 3.12 --seed
source .venv/bin/activate
uv pip install vllm --torch-backend=auto
```

### 六、 安装redis

```
#备份
mv ./redis.conf ./redis.conf.bak

#拷贝redis.conf到目录下
#有能力自己去看对应的帖子自行配置


#编译
sudo dnf install tcl -y

make

make test

#方案一（不配置手动操作）
cd ./src

redis-server --version
redis-cli --version

#方案二（安装 Redis 到系统目录）
sudo make install

redis-server --version
redis-cli --version

# 创建 Redis 安装目录
sudo mkdir -p /etc/redis
sudo mkdir -p /var/lib/redis
sudo mkdir -p /var/log/redis

sudo useradd -r -s /bin/false redis

sudo chown -R redis:redis /var/lib/redis
sudo chown -R redis:redis /var/log/redis
sudo chown -R redis:redis /etc/redis

# 复制默认配置文件
sudo cp redis.conf /etc/redis/redis.conf

sudo vi /etc/systemd/system/redis.service

[Unit]
Description=Redis In-Memory Data Store
After=network.target

[Service]
User=redis
Group=redis
Type=simple
ExecStart=/usr/local/bin/redis-server /etc/redis/redis.conf
ExecStop=/usr/local/bin/redis-cli -p 6379 shutdown
Restart=always
LimitNOFILE=10032

[Install]
WantedBy=multi-user.target


# 重新加载 systemd
sudo systemctl daemon-reexec
sudo systemctl daemon-reload

# 启动 Redis
sudo systemctl start redis

# 设置开机自启
sudo systemctl enable redis

# 查看状态
sudo systemctl status redis

#开放防火墙端口
sudo firewall-cmd --permanent --add-port=6379/tcp
sudo firewall-cmd --reload
```

### 七、 安装elasticsearch

```bash
#解压文件，确认目录后进行

#Elasticsearch 不能以 root 用户运行！
sudo useradd -r -s /bin/false elasticsearch
sudo chown -R elasticsearch:elasticsearch /opt/elasticsearch

#根据个人需求改变，仅供参考
#以下为整理结果，可能有问题

cd /opt/elasticsearch

#生成 CA 证书
sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearch-certutil ca \
  --out /opt/elasticsearch/config/certs/ca.p12 \
  --pass "123456" \
  -s

#生成节点证书

sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearch-certutil cert \
  --ca /opt/elasticsearch/config/certs/ca.p12 \
  --ca-pass "123456" \
  --name "node-1" \
  --ip "127.0.0.1" \
  --out /opt/elasticsearch/config/certs/elastic-certificates.p12 \
  --pass "123456" \
  -s
  
#sudo rm -f /opt/elasticsearch/config/certs/elastic-certificates.p12

#查看对应的部分文件
ls -l /opt/elasticsearch/config/certs/
#如：
#-rw-------. 1 elasticsearch elasticsearch 2688 Sep  6 06:44 ca.p12
#-rw-------. 1 elasticsearch elasticsearch 3608 Sep  6 07:46 elastic-certificates.p12



# 开放 9200 端口
sudo firewall-cmd --permanent --add-port=9200/tcp

# 重载防火墙
sudo firewall-cmd --reload

# 查看是否开放
sudo firewall-cmd --list-ports | grep 9200

#测试连接
sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearch-reset-password -u elastic
pQIdzZ-XdRlRNccNqjYO

curl -k -u elastic:你的密码 https://192.168.92.109:9200


#输出：
-rw-------. 1 elasticsearch elasticsearch 2688 Sep  6 06:44 ca.p12
-rw-------. 1 elasticsearch elasticsearch 3624 Sep  6 06:44 elastic-certificates.p12

sudo chown -R elasticsearch:elasticsearch /opt/elasticsearch/config/certs/
sudo chmod 600 /opt/elasticsearch/config/certs/*.p12

# 永久生效：写入配置
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf


sudo -u elasticsearch /opt/elasticsearch/bin/elasticsearch -d


#---------------------------参考配置-----------------------------

# =================================== Cluster ===================================
cluster.name: my-elastic-cluster
node.name: node-1
node.roles: [ data, master, ingest, ml ]

# =================================== Network ===================================
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300

# ================================ Discovery ====================================
discovery.type: single-node

# =============================== Security ======================================
xpack.security.enabled: true

xpack.security.http.ssl:
  enabled: true
  keystore.path: certs/elastic-certificates.p12
  keystore.password: "123456"

xpack.security.transport.ssl:
  enabled: true
  verification_mode: certificate
  keystore.path: certs/elastic-certificates.p12
  keystore.password: "123456"
  truststore.path: certs/elastic-certificates.p12
  truststore.password: "123456"
  
```

> 如果出了问题需要

### 八、安装milvus

> 注意python>3.8

```
pip install -U pymilvus
```

### 九、部署nacos

### 十、 部署kafka

该部分借鉴了[cscn](https://blog.csdn.net/weixin_42842069/article/details/149169442)

- 启动脚本

```bash
#!/bin/bash
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

if [ $# -lt 1 ];
then
        echo "USAGE: $0 [-daemon] server.properties [--override property=value]*"
        exit 1
fi
base_dir=$(dirname $0)

if [ "x$KAFKA_LOG4J_OPTS" = "x" ]; then
    export KAFKA_LOG4J_OPTS="-Dlog4j.configuration=file:$base_dir/../config/log4j.properties"
fi

if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi

EXTRA_ARGS=${EXTRA_ARGS-'-name kafkaServer -loggc'}

COMMAND=$1
case $COMMAND in
  -daemon)
    EXTRA_ARGS="-daemon "$EXTRA_ARGS
    shift
    ;;
  *)
    ;;
esac
# 添加server认证文件
export KAFKA_OPTS=" -Djava.security.auth.login.config=/opt/kafka/config/kafka_server_jaas.conf"
exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka "$@"
```

- server配置文件

```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#
# This configuration file is intended for use in KRaft mode, where
# Apache ZooKeeper is not present.
#

############################# Server Basics #############################

# The role of this server. Setting this puts us in KRaft mode
process.roles=broker,controller

# The node id associated with this instance's roles
node.id=1

# The connect string for the controller quorum
controller.quorum.voters=1@localhost:19093

############################# Socket Server Settings #############################

# The address the socket server listens on.
# Combined nodes (i.e. those with `process.roles=broker,controller`) must list the controller listener here at a minimum.
# If the broker listener is not defined, the default listener will use a host name that is equal to the value of java.net.InetAddress.getCanonicalHostName(),
# with PLAINTEXT listener name, and port 9092.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=PLAINTEXT://:19092,SASL_PLAINTEXT://:19094,CONTROLLER://:19093

# Name of listener used for communication between brokers.
# Listener name, hostname and port the broker or the controller will advertise to clients.
security.inter.broker.protocol=PLAINTEXT
sasl.kerberos.service.name=kafka
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN
# If not set, it uses the value for "listeners".
advertised.listeners=PLAINTEXT://localhost:19092,SASL_PLAINTEXT://172.28.135.99:19094,CONTROLLER://localhost:19093

# A comma-separated list of the names of the listeners used by the controller.
# If no explicit mapping set in `listener.security.protocol.map`, default will be using PLAINTEXT protocol
# This is required if running in KRaft mode.
controller.listener.names=CONTROLLER

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
log.dirs=/data/kafka/logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
log.retention.check.interval.ms=300000
```

- 服务端密码配置

```
KafkaServer {
    org.apache.kafka.common.security.plain.PlainLoginModule required
    username="admin"
    password="123456"
    user_admin="123456";
};
```

- 预备

```
/opt/kafka/bin/kafka-storage.sh random-uuid

#输入上面打印的
/opt/kafka/bin/kafka-storage.sh format -t XXXXXXXXXXXXXXXXXXX -c /opt/kafka/config/kraft/server.properties
```

- 启动脚本

```
/opt/kafka/bin/kafka-server-start-sasl.sh -daemon /opt/kafka/config/kraft/server_sasl.properties
```

- 停止脚本

```
/opt/kafka/bin/kafka-server-stop.sh
```

- topic操作

```
# 创建topic
bin/kafka-topics.sh --create --topic test-topic --bootstrap-server=localhost:19094 --command-config=config/kafka_client_jaas.conf
# 查看topic
bin/kafka-topics.sh --list --bootstrap-server=localhost:19094 --command-config=config/kafka_client_jaas.conf
# 删除topic
bin/kafka-topics.sh --delete --bootstrap-server=localhost:19094 --command-config=config/kafka_client_jaas.conf
```

- 客户端连接脚本

```
bootstrap.servers=localhost:19094
security.protocol=SASL_PLAINTEXT
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="admin" password="123456";
```

### 十一、 部署ollama

> 建议使用window本地部署，因为方便，有能力就使用vllm

### 十二、 部署rocketmq

### 十三、 部署zipkin

